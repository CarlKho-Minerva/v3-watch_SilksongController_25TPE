{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e19fbe",
   "metadata": {},
   "source": [
    "# WhisperX Transcription for Silksong Gesture Controller\n",
    "\n",
    "**Project:** Hollow Knight: Silksong Gesture Recognition\n",
    "\n",
    "**Purpose:** Transcribe audio recordings with word-level timestamps using WhisperX large-v3\n",
    "\n",
    "**Hardware:** GPU-accelerated (CUDA)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "2. **Upload audio files** to Google Drive: `My Drive/silksong_data/[session_name]/audio_16k.wav`\n",
    "3. **Run all cells** in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8085f87",
   "metadata": {},
   "source": [
    "## 1. Verify GPU Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d52c1",
   "metadata": {},
   "source": [
    "## 2. Install WhisperX and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install WhisperX\n",
    "!pip install -q git+https://github.com/m-bain/whisperx.git\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8645625e",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify mount\n",
    "import os\n",
    "base_path = '/content/drive/My Drive/silksong_data'\n",
    "\n",
    "if os.path.exists(base_path):\n",
    "    print(f\"‚úÖ Google Drive mounted successfully!\")\n",
    "    print(f\"\\nAvailable sessions:\")\n",
    "    sessions = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    for session in sessions:\n",
    "        print(f\"  - {session}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Path not found: {base_path}\")\n",
    "    print(f\"   Please create the folder structure in Google Drive first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5be433",
   "metadata": {},
   "source": [
    "## 4. Define Custom Prompt for Gesture Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt optimized for Silksong gesture recognition\n",
    "CUSTOM_PROMPT = (\n",
    "    \"The following is a transcription of a person playing the video game \"\n",
    "    \"Hollow Knight: Silksong. They are speaking their character's actions out loud. \"\n",
    "    \"The key commands are: jump, punch, attack, turn, walk, walking, walk start, \"\n",
    "    \"idle, rest, stop, noise. The speaker might say phrases like 'I'm gonna jump here', \"\n",
    "    \"'punch punch', 'let me walk over there', 'okay, now idle', or 'that was noise'.\"\n",
    ")\n",
    "\n",
    "print(\"Custom prompt configured:\")\n",
    "print(f\"  {CUSTOM_PROMPT[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a3157",
   "metadata": {},
   "source": [
    "## 5. Single Session Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE THIS: Set your session name\n",
    "SESSION_NAME = \"20251017_125600_session\"  # Change this to your session\n",
    "\n",
    "# Paths\n",
    "audio_path = f\"/content/drive/My Drive/silksong_data/{SESSION_NAME}/audio_16k.wav\"\n",
    "output_dir = f\"/content/drive/My Drive/silksong_data/{SESSION_NAME}/\"\n",
    "\n",
    "# Verify audio file exists\n",
    "if not os.path.exists(audio_path):\n",
    "    print(f\"‚ùå ERROR: Audio file not found: {audio_path}\")\n",
    "    print(f\"   Please upload audio_16k.wav to this location in Google Drive.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Audio file found: {audio_path}\")\n",
    "\n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(audio_path) / (1024 * 1024)\n",
    "    print(f\"   Size: {size_mb:.2f} MB\")\n",
    "\n",
    "    # Get duration (approximate)\n",
    "    import librosa\n",
    "    duration = librosa.get_duration(path=audio_path)\n",
    "    print(f\"   Duration: {duration/60:.2f} minutes\")\n",
    "    print(f\"   Estimated transcription time: {duration/60 * 0.5:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464edb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run WhisperX transcription\n",
    "import whisperx\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Starting WhisperX transcription...\\n\")\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"float32\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Compute type: {compute_type}\\n\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading WhisperX model (large-v3)...\")\n",
    "model = whisperx.load_model(\n",
    "    \"large-v3\",\n",
    "    device=device,\n",
    "    compute_type=compute_type,\n",
    "    language=\"en\"\n",
    ")\n",
    "print(\"‚úÖ Model loaded\\n\")\n",
    "\n",
    "# Load audio\n",
    "print(\"Loading audio...\")\n",
    "audio = whisperx.load_audio(audio_path)\n",
    "print(\"‚úÖ Audio loaded\\n\")\n",
    "\n",
    "# Transcribe with custom prompt\n",
    "print(\"Transcribing with custom gesture prompt...\")\n",
    "result = model.transcribe(\n",
    "    audio,\n",
    "    batch_size=16,\n",
    "    initial_prompt=CUSTOM_PROMPT\n",
    ")\n",
    "print(f\"‚úÖ Transcription complete!\")\n",
    "print(f\"   Segments: {len(result['segments'])}\")\n",
    "print(f\"   Language: {result.get('language', 'unknown')}\\n\")\n",
    "\n",
    "# Apply forced alignment for word-level timestamps\n",
    "print(\"Applying forced alignment for word-level timestamps...\")\n",
    "model_a, metadata = whisperx.load_align_model(\n",
    "    language_code=\"en\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "result = whisperx.align(\n",
    "    result[\"segments\"],\n",
    "    model_a,\n",
    "    metadata,\n",
    "    audio,\n",
    "    device,\n",
    "    return_char_alignments=False\n",
    ")\n",
    "\n",
    "# Count words\n",
    "word_count = sum(len(seg.get('words', [])) for seg in result.get('segments', []))\n",
    "print(f\"‚úÖ Alignment complete!\")\n",
    "print(f\"   Words with timestamps: {word_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "output_json = os.path.join(output_dir, \"whisperx_output.json\")\n",
    "output_txt = os.path.join(output_dir, \"whisperx_output_summary.txt\")\n",
    "\n",
    "# Save JSON\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úÖ Saved JSON: {output_json}\")\n",
    "\n",
    "# Save summary\n",
    "with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"WhisperX Transcription Summary\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    f.write(f\"Session: {SESSION_NAME}\\n\")\n",
    "    f.write(f\"Segments: {len(result.get('segments', []))}\\n\")\n",
    "    f.write(f\"Words: {word_count}\\n\\n\")\n",
    "\n",
    "    # First 50 words\n",
    "    f.write(\"First 50 words with timestamps:\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "\n",
    "    all_words = []\n",
    "    for seg in result.get('segments', []):\n",
    "        all_words.extend(seg.get('words', []))\n",
    "\n",
    "    for i, word_info in enumerate(all_words[:50], 1):\n",
    "        word = word_info.get('word', '')\n",
    "        start = word_info.get('start', 0)\n",
    "        end = word_info.get('end', 0)\n",
    "        score = word_info.get('score', 0)\n",
    "        f.write(f\"{i:3d}. {start:7.2f}s-{end:7.2f}s | {word:20s} | conf: {score:.3f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved summary: {output_txt}\")\n",
    "print(f\"\\nüéâ Transcription complete! Results saved to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5871a",
   "metadata": {},
   "source": [
    "## 6. Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few segments\n",
    "print(\"First 5 segments with word-level timestamps:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, segment in enumerate(result['segments'][:5], 1):\n",
    "    print(f\"\\n[Segment {i}] {segment['start']:.2f}s - {segment['end']:.2f}s\")\n",
    "    print(f\"Text: {segment['text']}\")\n",
    "\n",
    "    if 'words' in segment:\n",
    "        print(\"Words:\")\n",
    "        for word_info in segment['words']:\n",
    "            word = word_info['word']\n",
    "            start = word_info['start']\n",
    "            end = word_info['end']\n",
    "            conf = word_info.get('score', 0)\n",
    "            print(f\"  {start:6.2f}s-{end:6.2f}s: '{word}' (conf: {conf:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Total segments: {len(result['segments'])}\")\n",
    "print(f\"Total words: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e08867",
   "metadata": {},
   "source": [
    "## 7. Gesture Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536be9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count gesture keywords in transcription\n",
    "from collections import Counter\n",
    "\n",
    "gesture_keywords = ['jump', 'punch', 'attack', 'turn', 'walk', 'walking', 'idle', 'rest', 'stop', 'noise']\n",
    "\n",
    "# Extract all words\n",
    "all_words = []\n",
    "for seg in result['segments']:\n",
    "    for word_info in seg.get('words', []):\n",
    "        all_words.append(word_info['word'].lower().strip())\n",
    "\n",
    "# Count gesture keywords\n",
    "gesture_counts = Counter()\n",
    "for word in all_words:\n",
    "    if word in gesture_keywords:\n",
    "        gesture_counts[word] += 1\n",
    "\n",
    "# Display results\n",
    "print(\"Gesture Keyword Frequency Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal words transcribed: {len(all_words)}\")\n",
    "print(f\"Gesture keywords found: {sum(gesture_counts.values())}\")\n",
    "print(f\"Coverage: {sum(gesture_counts.values()) / len(all_words) * 100:.1f}%\\n\")\n",
    "\n",
    "print(\"Gesture breakdown:\")\n",
    "for keyword in sorted(gesture_keywords):\n",
    "    count = gesture_counts.get(keyword, 0)\n",
    "    if count > 0:\n",
    "        bar = '‚ñà' * int(count / 2)\n",
    "        print(f\"  {keyword:10s}: {count:3d} {bar}\")\n",
    "    else:\n",
    "        print(f\"  {keyword:10s}: {count:3d}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2c28d",
   "metadata": {},
   "source": [
    "## 8. Batch Processing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE THIS: List of session names to process\n",
    "SESSIONS_TO_PROCESS = [\n",
    "    \"20251017_125600_session\",\n",
    "    # \"20251017_130000_session\",\n",
    "    # \"20251017_131500_session\",\n",
    "    # Add more session names here\n",
    "]\n",
    "\n",
    "base_path = \"/content/drive/My Drive/silksong_data/\"\n",
    "\n",
    "print(f\"Batch processing {len(SESSIONS_TO_PROCESS)} session(s)\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, session in enumerate(SESSIONS_TO_PROCESS, 1):\n",
    "    print(f\"\\n[{i}/{len(SESSIONS_TO_PROCESS)}] Processing: {session}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    audio_path = os.path.join(base_path, session, \"audio_16k.wav\")\n",
    "    output_dir = os.path.join(base_path, session)\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"‚ö†Ô∏è  Skipping - audio file not found\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Transcribe\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        result = model.transcribe(audio, batch_size=16, initial_prompt=CUSTOM_PROMPT)\n",
    "\n",
    "        # Align\n",
    "        result = whisperx.align(\n",
    "            result[\"segments\"],\n",
    "            model_a,\n",
    "            metadata,\n",
    "            audio,\n",
    "            device,\n",
    "            return_char_alignments=False\n",
    "        )\n",
    "\n",
    "        # Save\n",
    "        output_json = os.path.join(output_dir, \"whisperx_output.json\")\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        word_count = sum(len(seg.get('words', [])) for seg in result.get('segments', []))\n",
    "        print(f\"‚úÖ Complete - {len(result['segments'])} segments, {word_count} words\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ Batch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064f0dc",
   "metadata": {},
   "source": [
    "## 9. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the JSON file directly to your computer\n",
    "from google.colab import files\n",
    "\n",
    "# Download the transcription results\n",
    "output_json = os.path.join(output_dir, \"whisperx_output.json\")\n",
    "\n",
    "if os.path.exists(output_json):\n",
    "    print(f\"Downloading: {output_json}\")\n",
    "    files.download(output_json)\n",
    "    print(\"‚úÖ Download started! Check your browser's download folder.\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe00208",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After transcription completes:\n",
    "\n",
    "1. **Download** the `whisperx_output.json` file from Google Drive\n",
    "2. **Move** to your local project: `data/continuous/[session_name]/whisperx_output.json`\n",
    "3. **Run label alignment** on your local machine:\n",
    "   ```bash\n",
    "   python align_voice_labels.py \\\n",
    "     --session [session_name] \\\n",
    "     --whisper data/continuous/[session_name]/whisperx_output.json\n",
    "   ```\n",
    "4. **Continue** with Phase III training data preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Project Documentation:** `docs/Phase_V/CLOUD_GPU_GUIDE.md`\n",
    "- **WhisperX GitHub:** https://github.com/m-bain/whisperx\n",
    "- **Colab FAQ:** https://research.google.com/colaboratory/faq.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
