{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS156: Silksong Watch Controller - Machine Learning Pipeline\n",
    "\n",
    "**Phase III: From Raw Sensor Data to Gesture Classification Model**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook documents the complete machine learning pipeline for transforming raw IMU sensor data from a smartwatch into a functional gesture recognition system for controlling Hollow Knight/Silksong.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. **Data Loading & Exploration** - Understanding the raw sensor data\n",
    "2. **Feature Engineering** - Extracting meaningful features from time-series data\n",
    "3. **Model Training** - Training and evaluating classification models\n",
    "4. **Model Serialization** - Saving models for deployment\n",
    "5. **Validation** - Testing model performance\n",
    "\n",
    "**Model Choice:** Support Vector Machine (SVM) with RBF kernel\n",
    "- Mathematical rigor and explainability\n",
    "- Strong performance on high-dimensional feature spaces\n",
    "- Efficient for real-time prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and configure notebook settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Signal processing\n",
    "from scipy import stats\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Notebook settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Environment setup complete\")\n",
    "print(f\"  Python: {pd.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "### 2.1 Understanding the Data Format\n",
    "\n",
    "**Data Structure:**\n",
    "```\n",
    "training_data/\n",
    "└── session_YYYYMMDD_HHMMSS/\n",
    "    ├── gesture_name_sample01.csv\n",
    "    ├── gesture_name_sample02.csv\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "**CSV Format:**\n",
    "- `timestamp`: Time in seconds (0.000 to 3.000)\n",
    "- `sensor`: Type (rotation_vector, linear_acceleration, gyroscope)\n",
    "- `gesture`: Gesture label\n",
    "- `stance`: Physical stance (combat, neutral, travel)\n",
    "- `sample`: Sample number (1-5)\n",
    "- Sensor-specific columns: rot_x/y/z/w, accel_x/y/z, gyro_x/y/z\n",
    "\n",
    "**Sensors Used:**\n",
    "1. **Linear Acceleration** - Linear movement (gravity removed)\n",
    "2. **Gyroscope** - Angular velocity\n",
    "3. **Rotation Vector** - Orientation (quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_data(session_dir):\n",
    "    \"\"\"Load all CSV files from a data collection session.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    session_dir : str\n",
    "        Path to session directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with all recordings from the session\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(session_dir, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"⚠️  No CSV files found in {session_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data_frames = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Add source filename for tracking\n",
    "            df['source_file'] = os.path.basename(csv_file)\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading {csv_file}: {e}\")\n",
    "    \n",
    "    if not data_frames:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all recordings\n",
    "    combined = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_training_data(base_dir=\"training_data\"):\n",
    "    \"\"\"Load training data from all sessions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing session folders\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with all training data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"❌ Training data directory not found: {base_dir}\")\n",
    "        print(\"\\nPlease run the data collection script first:\")\n",
    "        print(\"  python src/data_collector.py\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    session_dirs = glob.glob(os.path.join(base_dir, \"session_*\"))\n",
    "    \n",
    "    if not session_dirs:\n",
    "        print(f\"❌ No session directories found in {base_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"📁 Found {len(session_dirs)} session(s)\")\n",
    "    \n",
    "    all_sessions = []\n",
    "    for session_dir in sorted(session_dirs):\n",
    "        session_name = os.path.basename(session_dir)\n",
    "        print(f\"  Loading {session_name}...\")\n",
    "        \n",
    "        session_data = load_session_data(session_dir)\n",
    "        if not session_data.empty:\n",
    "            session_data['session'] = session_name\n",
    "            all_sessions.append(session_data)\n",
    "    \n",
    "    if not all_sessions:\n",
    "        print(\"❌ No data loaded from any session\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all sessions\n",
    "    all_data = pd.concat(all_sessions, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n✓ Loaded {len(all_data):,} sensor readings\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all training data\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_data = load_all_training_data()\n",
    "\n",
    "if all_data.empty:\n",
    "    print(\"\\n⚠️  No training data available. Cannot proceed with model training.\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Run the data collection script: python src/data_collector.py\")\n",
    "    print(\"2. Follow the guided procedure to collect gesture data\")\n",
    "    print(\"3. Re-run this notebook after data collection\")\n",
    "else:\n",
    "    print(\"\\n✓ Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if data was loaded\n",
    "if not all_data.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nDataset Shape: {all_data.shape}\")\n",
    "    print(f\"  Rows (sensor readings): {all_data.shape[0]:,}\")\n",
    "    print(f\"  Columns: {all_data.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Gestures ---\")\n",
    "    gesture_counts = all_data.groupby('gesture')['sample'].nunique().sort_values(ascending=False)\n",
    "    print(f\"Unique gestures: {len(gesture_counts)}\")\n",
    "    for gesture, count in gesture_counts.items():\n",
    "        print(f\"  {gesture}: {count} samples\")\n",
    "    \n",
    "    print(\"\\n--- Sensors ---\")\n",
    "    sensor_counts = all_data['sensor'].value_counts()\n",
    "    for sensor, count in sensor_counts.items():\n",
    "        print(f\"  {sensor}: {count:,} readings\")\n",
    "    \n",
    "    print(\"\\n--- Stances ---\")\n",
    "    stance_counts = all_data['stance'].value_counts()\n",
    "    for stance, count in stance_counts.items():\n",
    "        print(f\"  {stance}: {count:,} readings\")\n",
    "    \n",
    "    print(\"\\n--- Data Quality ---\")\n",
    "    missing_data = all_data.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"Missing values detected:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    else:\n",
    "        print(\"No missing values in metadata columns\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n--- Sample Data ---\")\n",
    "    display(all_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gesture distribution\n",
    "    gesture_samples = all_data.groupby('gesture')['sample'].nunique().sort_values(ascending=False)\n",
    "    axes[0].bar(range(len(gesture_samples)), gesture_samples.values)\n",
    "    axes[0].set_xticks(range(len(gesture_samples)))\n",
    "    axes[0].set_xticklabels(gesture_samples.index, rotation=45, ha='right')\n",
    "    axes[0].set_xlabel('Gesture')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    axes[0].set_title('Samples per Gesture')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Sensor distribution\n",
    "    sensor_counts = all_data['sensor'].value_counts()\n",
    "    axes[1].bar(range(len(sensor_counts)), sensor_counts.values)\n",
    "    axes[1].set_xticks(range(len(sensor_counts)))\n",
    "    axes[1].set_xticklabels(sensor_counts.index, rotation=45, ha='right')\n",
    "    axes[1].set_xlabel('Sensor Type')\n",
    "    axes[1].set_ylabel('Number of Readings')\n",
    "    axes[1].set_title('Sensor Reading Distribution')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Feature Extraction Strategy\n",
    "\n",
    "We extract features from time windows of sensor data. For each gesture sample (3 seconds of recording), we compute:\n",
    "\n",
    "**Time-Domain Features:**\n",
    "- Statistical moments: mean, std, min, max, range\n",
    "- Distribution shape: skewness, kurtosis\n",
    "- Peak detection: number of peaks above threshold\n",
    "\n",
    "**Frequency-Domain Features:**\n",
    "- FFT magnitude spectrum\n",
    "- Dominant frequency components\n",
    "- Spectral energy\n",
    "\n",
    "**Per-Sensor Features:**\n",
    "- Acceleration (x, y, z): 10 features per axis = 30 features\n",
    "- Gyroscope (x, y, z): 8 features per axis = 24 features\n",
    "- Rotation (quaternion): 8 features = 8 features\n",
    "\n",
    "**Total: ~60+ features per gesture sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_window_features(window_df):\n",
    "    \"\"\"Extract comprehensive features from a time window of sensor data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    window_df : pd.DataFrame\n",
    "        DataFrame containing sensor readings for a single gesture sample\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of extracted features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Separate by sensor type\n",
    "    accel = window_df[window_df['sensor'] == 'linear_acceleration']\n",
    "    gyro = window_df[window_df['sensor'] == 'gyroscope']\n",
    "    rot = window_df[window_df['sensor'] == 'rotation_vector']\n",
    "    \n",
    "    # ========== ACCELERATION FEATURES ==========\n",
    "    if len(accel) > 0:\n",
    "        for axis in ['accel_x', 'accel_y', 'accel_z']:\n",
    "            values = accel[axis].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                # Time-domain statistics\n",
    "                features[f'{axis}_mean'] = values.mean()\n",
    "                features[f'{axis}_std'] = values.std()\n",
    "                features[f'{axis}_max'] = values.max()\n",
    "                features[f'{axis}_min'] = values.min()\n",
    "                features[f'{axis}_range'] = values.max() - values.min()\n",
    "                features[f'{axis}_median'] = values.median()\n",
    "                \n",
    "                # Distribution shape\n",
    "                features[f'{axis}_skew'] = stats.skew(values)\n",
    "                features[f'{axis}_kurtosis'] = stats.kurtosis(values)\n",
    "                \n",
    "                # Peak detection\n",
    "                threshold = values.mean() + 2 * values.std()\n",
    "                features[f'{axis}_peak_count'] = len(values[values > threshold])\n",
    "                \n",
    "                # Frequency domain (FFT)\n",
    "                if len(values) > 2:\n",
    "                    fft_vals = np.abs(fft(values))[:len(values)//2]\n",
    "                    if len(fft_vals) > 0:\n",
    "                        features[f'{axis}_fft_max'] = fft_vals.max()\n",
    "                        features[f'{axis}_dominant_freq'] = fft_vals.argmax()\n",
    "                        features[f'{axis}_fft_mean'] = fft_vals.mean()\n",
    "    \n",
    "    # ========== GYROSCOPE FEATURES ==========\n",
    "    if len(gyro) > 0:\n",
    "        for axis in ['gyro_x', 'gyro_y', 'gyro_z']:\n",
    "            values = gyro[axis].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                # Time-domain statistics\n",
    "                features[f'{axis}_mean'] = values.mean()\n",
    "                features[f'{axis}_std'] = values.std()\n",
    "                features[f'{axis}_max_abs'] = values.abs().max()\n",
    "                features[f'{axis}_range'] = values.max() - values.min()\n",
    "                \n",
    "                # Distribution shape\n",
    "                features[f'{axis}_skew'] = stats.skew(values)\n",
    "                features[f'{axis}_kurtosis'] = stats.kurtosis(values)\n",
    "                \n",
    "                # RMS (root mean square)\n",
    "                features[f'{axis}_rms'] = np.sqrt(np.mean(values**2))\n",
    "                \n",
    "                # Frequency domain\n",
    "                if len(values) > 2:\n",
    "                    fft_vals = np.abs(fft(values))[:len(values)//2]\n",
    "                    if len(fft_vals) > 0:\n",
    "                        features[f'{axis}_fft_max'] = fft_vals.max()\n",
    "    \n",
    "    # ========== ROTATION FEATURES ==========\n",
    "    if len(rot) > 0:\n",
    "        for axis in ['rot_x', 'rot_y', 'rot_z', 'rot_w']:\n",
    "            values = rot[axis].dropna()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                features[f'{axis}_mean'] = values.mean()\n",
    "                features[f'{axis}_std'] = values.std()\n",
    "                features[f'{axis}_range'] = values.max() - values.min()\n",
    "    \n",
    "    # ========== CROSS-SENSOR FEATURES ==========\n",
    "    # Acceleration magnitude\n",
    "    if len(accel) > 0:\n",
    "        accel_mag = np.sqrt(\n",
    "            accel['accel_x'].fillna(0)**2 + \n",
    "            accel['accel_y'].fillna(0)**2 + \n",
    "            accel['accel_z'].fillna(0)**2\n",
    "        )\n",
    "        features['accel_magnitude_mean'] = accel_mag.mean()\n",
    "        features['accel_magnitude_max'] = accel_mag.max()\n",
    "        features['accel_magnitude_std'] = accel_mag.std()\n",
    "    \n",
    "    # Gyroscope magnitude\n",
    "    if len(gyro) > 0:\n",
    "        gyro_mag = np.sqrt(\n",
    "            gyro['gyro_x'].fillna(0)**2 + \n",
    "            gyro['gyro_y'].fillna(0)**2 + \n",
    "            gyro['gyro_z'].fillna(0)**2\n",
    "        )\n",
    "        features['gyro_magnitude_mean'] = gyro_mag.mean()\n",
    "        features['gyro_magnitude_max'] = gyro_mag.max()\n",
    "        features['gyro_magnitude_std'] = gyro_mag.std()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Apply Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXTRACTING FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Group by gesture and sample to process each recording\n",
    "    unique_gestures = all_data['gesture'].unique()\n",
    "    total_samples = 0\n",
    "    \n",
    "    for gesture in unique_gestures:\n",
    "        gesture_data = all_data[all_data['gesture'] == gesture]\n",
    "        \n",
    "        # Get unique samples for this gesture\n",
    "        unique_samples = gesture_data['sample'].unique()\n",
    "        \n",
    "        for sample in unique_samples:\n",
    "            # Extract data for this specific sample\n",
    "            sample_data = gesture_data[gesture_data['sample'] == sample]\n",
    "            \n",
    "            # Extract features from this sample\n",
    "            features = extract_window_features(sample_data)\n",
    "            features_list.append(features)\n",
    "            labels_list.append(gesture)\n",
    "            total_samples += 1\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = pd.DataFrame(features_list).fillna(0)\n",
    "    y = pd.Series(labels_list)\n",
    "    \n",
    "    print(f\"\\n✓ Feature extraction complete!\")\n",
    "    print(f\"  Processed samples: {total_samples}\")\n",
    "    print(f\"  Feature matrix shape: {X.shape}\")\n",
    "    print(f\"  Number of features: {X.shape[1]}\")\n",
    "    print(f\"  Number of classes: {y.nunique()}\")\n",
    "    \n",
    "    print(\"\\n--- Class Distribution ---\")\n",
    "    class_dist = y.value_counts().sort_index()\n",
    "    for label, count in class_dist.items():\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    \n",
    "    print(\"\\n--- Sample Features (first 10) ---\")\n",
    "    display(X.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### 4.1 Train-Test Split\n",
    "\n",
    "We use stratified splitting to ensure balanced class representation in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'X' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PREPARING TRAINING AND TEST SETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Split data (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    print(\"\\n--- Training Set Distribution ---\")\n",
    "    train_dist = y_train.value_counts().sort_index()\n",
    "    for label, count in train_dist.items():\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    \n",
    "    print(\"\\n--- Test Set Distribution ---\")\n",
    "    test_dist = y_test.value_counts().sort_index()\n",
    "    for label, count in test_dist.items():\n",
    "        print(f\"  {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Scaling\n",
    "\n",
    "Normalize features using StandardScaler for optimal SVM performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'X_train' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCALING FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data and transform both sets\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\n✓ Features scaled using StandardScaler\")\n",
    "    print(f\"  Mean of scaled features: {X_train_scaled.mean():.6f}\")\n",
    "    print(f\"  Std of scaled features: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Training - Support Vector Machine\n",
    "\n",
    "**Model Choice: SVM with RBF Kernel**\n",
    "\n",
    "Advantages:\n",
    "- Excellent for high-dimensional feature spaces\n",
    "- Non-linear decision boundaries via RBF kernel\n",
    "- Strong theoretical foundations\n",
    "- Good generalization with proper hyperparameter tuning\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- `C`: Regularization parameter\n",
    "- `gamma`: Kernel coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'X_train_scaled' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING SVM MODEL WITH HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "    \n",
    "    print(\"\\nSearching optimal hyperparameters...\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    print(f\"Total combinations: {len(param_grid['C']) * len(param_grid['gamma'])}\")\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        SVC(random_state=RANDOM_SEED, probability=True),  # Enable probability estimates\n",
    "        param_grid,\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    svm_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GRID SEARCH RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n✓ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"✓ Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Show top 5 parameter combinations\n",
    "    print(\"\\n--- Top 5 Parameter Combinations ---\")\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    top_results = results_df.nlargest(5, 'mean_test_score')[[\n",
    "        'params', 'mean_test_score', 'std_test_score'\n",
    "    ]]\n",
    "    for idx, row in top_results.iterrows():\n",
    "        print(f\"  {row['params']}\")\n",
    "        print(f\"    Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Alternative Model - Random Forest\n",
    "\n",
    "For comparison, we also train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'X_train_scaled' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING RANDOM FOREST MODEL (FOR COMPARISON)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"\\n✓ Random Forest trained\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'svm_model' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SVM MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "    y_pred_proba_svm = svm_model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "    f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTest Set Accuracy: {accuracy_svm:.4f}\")\n",
    "    print(f\"Test Set F1-Score (weighted): {f1_svm:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred_svm))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred_svm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=sorted(y.unique()),\n",
    "        yticklabels=sorted(y.unique())\n",
    "    )\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('SVM Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'rf_model' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RANDOM FOREST MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTest Set Accuracy: {accuracy_rf:.4f}\")\n",
    "    print(f\"Test Set F1-Score (weighted): {f1_rf:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cross-Validation\n",
    "\n",
    "Perform additional cross-validation on the full dataset to validate model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'svm_model' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Scale all data\n",
    "    X_all_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform cross-validation with best SVM model\n",
    "    cv_scores = cross_val_score(\n",
    "        svm_model,\n",
    "        X_all_scaled,\n",
    "        y,\n",
    "        cv=5,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n5-Fold Cross-Validation Results:\")\n",
    "    print(f\"  Scores: {cv_scores}\")\n",
    "    print(f\"  Mean accuracy: {cv_scores.mean():.4f}\")\n",
    "    print(f\"  Std deviation: {cv_scores.std():.4f}\")\n",
    "    print(f\"  95% confidence interval: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, {cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'svm_model' in locals() and 'rf_model' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['SVM (RBF)', 'Random Forest'],\n",
    "        'Accuracy': [accuracy_svm, accuracy_rf],\n",
    "        'F1-Score': [f1_svm, f1_rf]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Model Accuracy Comparison')\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    axes[1].bar(comparison_df['Model'], comparison_df['F1-Score'])\n",
    "    axes[1].set_ylabel('F1-Score')\n",
    "    axes[1].set_title('Model F1-Score Comparison')\n",
    "    axes[1].set_ylim([0, 1.0])\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine best model\n",
    "    best_model_name = 'SVM' if accuracy_svm >= accuracy_rf else 'Random Forest'\n",
    "    best_model = svm_model if accuracy_svm >= accuracy_rf else rf_model\n",
    "    \n",
    "    print(f\"\\n✓ Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Serialization\n",
    "\n",
    "Save the trained model and scaler for deployment in the real-time controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and 'svm_model' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAVING MODELS FOR DEPLOYMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save SVM model\n",
    "    model_path = 'models/gesture_classifier.pkl'\n",
    "    joblib.dump(svm_model, model_path)\n",
    "    print(f\"\\n✓ Model saved: {model_path}\")\n",
    "    print(f\"  File size: {os.path.getsize(model_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = 'models/feature_scaler.pkl'\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"\\n✓ Scaler saved: {scaler_path}\")\n",
    "    print(f\"  File size: {os.path.getsize(scaler_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Save feature names for reference\n",
    "    feature_names_path = 'models/feature_names.pkl'\n",
    "    joblib.dump(X.columns.tolist(), feature_names_path)\n",
    "    print(f\"\\n✓ Feature names saved: {feature_names_path}\")\n",
    "    \n",
    "    # Save Random Forest as alternative\n",
    "    if 'rf_model' in locals():\n",
    "        rf_path = 'models/random_forest_classifier.pkl'\n",
    "        joblib.dump(rf_model, rf_path)\n",
    "        print(f\"\\n✓ Random Forest saved: {rf_path}\")\n",
    "        print(f\"  File size: {os.path.getsize(rf_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'model_type': 'SVM',\n",
    "        'kernel': 'rbf',\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'test_accuracy': accuracy_svm,\n",
    "        'test_f1_score': f1_svm,\n",
    "        'cv_mean_score': cv_scores.mean(),\n",
    "        'cv_std_score': cv_scores.std(),\n",
    "        'num_features': X.shape[1],\n",
    "        'num_classes': y.nunique(),\n",
    "        'classes': sorted(y.unique()),\n",
    "        'training_samples': len(y),\n",
    "        'training_date': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    metadata_path = 'models/model_metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"\\n✓ Model metadata saved: {metadata_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Models are ready for deployment\")\n",
    "    print(\"2. Update src/udp_listener.py to load these models\")\n",
    "    print(\"3. Implement real-time gesture prediction\")\n",
    "    print(\"4. Test with live sensor data from the smartwatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Validation\n",
    "\n",
    "Test model loading and prediction to ensure deployment readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_data.empty and os.path.exists('models/gesture_classifier.pkl'):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VALIDATING MODEL DEPLOYMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load saved model and scaler\n",
    "    loaded_model = joblib.load('models/gesture_classifier.pkl')\n",
    "    loaded_scaler = joblib.load('models/feature_scaler.pkl')\n",
    "    loaded_features = joblib.load('models/feature_names.pkl')\n",
    "    \n",
    "    print(\"\\n✓ Models loaded successfully\")\n",
    "    \n",
    "    # Test prediction on a sample\n",
    "    test_sample = X_test.iloc[0:1]\n",
    "    test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "    \n",
    "    prediction = loaded_model.predict(test_sample_scaled)\n",
    "    confidence = loaded_model.predict_proba(test_sample_scaled).max()\n",
    "    \n",
    "    print(\"\\n--- Test Prediction ---\")\n",
    "    print(f\"  Predicted gesture: {prediction[0]}\")\n",
    "    print(f\"  Confidence: {confidence:.4f}\")\n",
    "    print(f\"  Actual gesture: {y_test.iloc[0]}\")\n",
    "    print(f\"  Correct: {'✓' if prediction[0] == y_test.iloc[0] else '✗'}\")\n",
    "    \n",
    "    # Verify feature names match\n",
    "    if loaded_features == X.columns.tolist():\n",
    "        print(\"\\n✓ Feature names match\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Feature name mismatch detected\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ MODEL VALIDATION SUCCESSFUL\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### Pipeline Summary\n",
    "\n",
    "This notebook has completed the following:\n",
    "\n",
    "1. ✓ **Data Loading** - Loaded and validated IMU sensor data\n",
    "2. ✓ **Feature Engineering** - Extracted 60+ features from time-series data\n",
    "3. ✓ **Model Training** - Trained SVM and Random Forest classifiers\n",
    "4. ✓ **Hyperparameter Tuning** - Optimized model parameters via GridSearchCV\n",
    "5. ✓ **Model Evaluation** - Validated performance on test set\n",
    "6. ✓ **Model Serialization** - Saved models for deployment\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "- **Target Accuracy**: >85% on test set\n",
    "- **Real-time Latency**: <100ms per prediction\n",
    "- **Confidence Threshold**: 0.7 (70%) for gesture execution\n",
    "\n",
    "### Phase IV: Real-Time Deployment\n",
    "\n",
    "To integrate this model into the game controller:\n",
    "\n",
    "1. **Update `src/udp_listener.py`**:\n",
    "   ```python\n",
    "   import joblib\n",
    "   from collections import deque\n",
    "   \n",
    "   # Load models\n",
    "   model = joblib.load('models/gesture_classifier.pkl')\n",
    "   scaler = joblib.load('models/feature_scaler.pkl')\n",
    "   \n",
    "   # Create buffer for sensor data\n",
    "   sensor_buffer = deque(maxlen=150)  # ~3 seconds at 50Hz\n",
    "   ```\n",
    "\n",
    "2. **Implement prediction logic**:\n",
    "   - Collect sensor data in a sliding window\n",
    "   - Extract features using `extract_window_features()`\n",
    "   - Scale features using loaded scaler\n",
    "   - Predict gesture with confidence threshold\n",
    "   - Execute corresponding keyboard action\n",
    "\n",
    "3. **Test with live data**:\n",
    "   - Run data collector to stream sensor data\n",
    "   - Verify real-time predictions\n",
    "   - Tune confidence thresholds for responsiveness\n",
    "\n",
    "### Watson Preferred Checklist\n",
    "\n",
    "- [x] Clear problem statement and objectives\n",
    "- [x] Data loading and exploration\n",
    "- [x] Feature engineering with justification\n",
    "- [x] Model selection with rationale\n",
    "- [x] Hyperparameter tuning methodology\n",
    "- [x] Comprehensive evaluation metrics\n",
    "- [x] Model serialization for deployment\n",
    "- [x] Validation and testing\n",
    "- [x] Documentation and next steps\n",
    "\n",
    "---\n",
    "\n",
    "**End of ML Pipeline Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
